{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Pose of Human Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Group 12: Cai Yujun, Song Guoxian, Zhang Zhijie*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation and Description\n",
    "Image-based 3D face pose recovery is a fundamental problem in computer vision and graphics, with many applications such as face recognition[$^{[1]}$][TOC] and face animation[$^{[2]}$][TOC]. Different from many traditional facial performance capture methods which require complex hardware and significant user intervention[$^{[3]}$][TOC][$^{[4]}$][TOC], we aim at utilizing the powerfulness of convolutional neural networks (CNN) to realize a real-time framework for face pose recovery.\n",
    "\n",
    "Our work focuses on the task of 3D face pose recovery from a single face image. We plan to tackle this problem with the help of a robust facial pose regression network (ResNet 18[$^{[5]}$][TOC]), which tries to regress the rotation, translation and scale information of the target face. In general, we proposed a robust facial pose regression network (CNN) and regress the rotation, translation and scale information of the target face.\n",
    "\n",
    "[TOC]: #Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration and Pre-process\n",
    "For robust Face pose net, we need different subjects under different environment lighting, skin colors, poses, expressions and various ages. There are a various publicly available facial datasets. After exploration, we find a suitable dataset which there are about 3131 subjects from different environment and expressions. We first download a face dataset called “CoarseData[$^{[6]}$][TOC]”  from a public source. This dataset includes about 60000 images. For each image, labels such as 3D face rotation labels, scale, translation label respect to the original image coordinate are provided in the dataset.\n",
    "\n",
    "For the public face dataset \"CoarseData\", we find that the dataset is already augmented by the authors. They use a 3D model to fit the original image and changes face expression in 3D spaze and image wrap into 2D space. The total size is about 80000, 450x450 RGB facial images. Therefore, at this stage, we already get the raw RGB data with one person's face in each image, and the corresponding rotation, scale, translation labels as the ground truth labels for our training process.\n",
    "\n",
    "In terms of data pre-processing, the main task is to normalize the facial images into the same size and put the face in the middle. First, we use `dlib` landmark detector to detect face location in the image and its corresponding 68 facial landmarks. Second, we compute the face size from the landmarks by calculating the difference between maximum and minimum coordinates in $x$ and $y$ directions. The image is then focused by a window that has same centre with detected face centre and is 1.2 times larger than the face size. In order to prevent to distort the images, we uniformly scale and crop the image into 224x224 size. Finally, we normalise and scale the data. Some of the function codes of these steps can be found below.\n",
    "\n",
    "- Wrapped functions of the pre-processing of one image\n",
    "\n",
    "```Python\n",
    "test_img = cv2.imread(os.path.join(path,'%d.jpg'%(i_frame)))\n",
    "resized_img = FacePoseNet.Normalize(img=test_img)\n",
    "eular, t, s = FacePoseNet.OneFrame(resized_img=resized_img)\n",
    "```\n",
    "- Change RGB images into gray ones and send into detector to get 68 facial landmarks\n",
    "\n",
    "```Python\n",
    "import dlib\n",
    "self.detector = dlib.get_frontal_face_detector()\n",
    "self.predictor = dlib.shape_predictor(predictor_path)\n",
    "gray_ = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "dets = self.detector(gray_, 1)\n",
    "```\n",
    "- Normalize the facial images into the same size and put the face in the middle. \n",
    "\n",
    "```Python\n",
    "# get the landmarks position range and find the center of the human face\n",
    "xmin = np.min(landmark_[:, 0])\n",
    "xmax = np.max(landmark_[:, 0])\n",
    "ymin = np.min(landmark_[:, 1])\n",
    "ymax = np.max(landmark_[:, 1])\n",
    "old_cx = (xmin + xmax) / 2\n",
    "old_cy = (ymin + ymax) / 2\n",
    "\n",
    "# compute the face size and decide the resize scale of original image\n",
    "length = ((xmax - xmin) ** 2 + (ymax - ymin) ** 2) ** 0.5\n",
    "length *= 1.2\n",
    "ori_crop_scale = 224 / length\n",
    "\n",
    "# Uniformly scale every images and crop into 224x224 size\n",
    "image = cv2.resize(img, (0, 0), fx=ori_crop_scale, fy=ori_crop_scale)\n",
    "old_cx = old_cx * ori_crop_scale\n",
    "old_cy = old_cy * ori_crop_scale\n",
    "start_x = int(old_cx - cx)\n",
    "start_y = int(old_cy - cy)\n",
    "crop_image = image[start_y:start_y + 224, start_x:start_x + 224]\n",
    "shape_ = np.shape(crop_image)\n",
    "resized_img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "resized_img[:shape_[0], :shape_[1], :] = crop_image\n",
    "```\n",
    "\n",
    "[TOC]: #Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Proposed Solutions: ResNet 18\n",
    "3D human pose regression is a challenging problem because different person has different skin tones and face shapes. Traditional methods like using sparse key points detection to solve perspective-n-point (PnP) problem to fit a 3D template are sensitive to key points detection. Recent methods like inverse rendering, which uses 3D morphable model as base and get affine translation matrix by solving a complex optimization, are time-consuming and hard to achieve real-time speed.\n",
    "\n",
    "There are various CNN based network structures, like LeNet[$^{[6]}$][TOC], VGG[$^{[7]}$][TOC] and ResNet[$^{[5]}$][TOC]. We reviewed relative papers about the 3D facial regression problem. Since different person has different faces, we considered to use ResNet, a powerful network structure. ResNet is a popular and powerful network structure compared to other networks. When traditional networks start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated and then degrades rapidly. ResNet solves this problem by reformulating the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.\n",
    "\n",
    "Concretely, instead of learning a direct mapping from inputs to outputs with a function $H(x)$ (A few stacked non-linear layers). A residual function is defined as $F(x)=H(x)−x$, which can be reframed into $H(x)=F(x)+x$, where $F(x)$ and $x$ represents the stacked non-linear layers and the identity function respectively. If the identity mapping is optimal, it is easy to push the residuals to zero ($F(x)=0$) than to fit an identity mapping by a stack of non-linear layers. In other word, the network introduces a simple way to come up with a solution like $F(x)=0$ rather than $F(x)=x$ using stack of non-linear CNN layers as function. \n",
    "\n",
    "We utilize a shallow residual learning block in our model. The building block is defines as: \n",
    "$$y=\\cal{F} \\mit(x,{W_i})+x.$$\n",
    "\n",
    "$\\cal{F} \\mit(x,{W_i})$ is the residual mapping, which can be written as $\\cal{F} \\mit=W_2\\sigma(W_1x)$ and $\\sigma$ denotes the Relu function. The identity mapping in residual blocks are shown in Fig. 1.\n",
    "\n",
    "![Fig.1](images/Fig1.png)\n",
    "Fig. 1. Plain block (left) and residual block (right) in traditional model and ResNet model. \n",
    "\n",
    "ResNet has several CNN layers to extract image features and a res-block to increase the depth of the network. The Network architectures are shown in Fig. 2. There are 18 layers in the model including one input layer and one output layer. \n",
    "\n",
    "![Fig.2](images/Fig2.png)\n",
    "Fig. 2. The model architecture of ResNet 18.\n",
    "\n",
    "The model is trained on the 66336 training images with 3131 subjects, and evaluated on the 5000 images. Our input is RGB pictures with size of 224$\\times$224. As for the test, we also use our own faces to test the robustness, which means that there will be no accurate pose label, but visualized performance.\n",
    "\n",
    "[TOC]: #Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Result Analysis\n",
    "As mentioned previously, we have trained our model on 66336-image dataset (including 3131 subjects) and then evaluated model performance on test set with 5000 images. The results are acceptable. The mean absolute errors (MAE) for translation, rotation and scale are listed in Table 1. Fig. 3. is a collection of some results. \n",
    "\n",
    "| $\\quad$ | Translation(Pixel)  |   Eular Rotation    |        Scale          |\n",
    "| :-----: | :-----------------: | :-----------------: | :-------------------: |\n",
    "|   MAE   |  7.315566184583461  | 0.08901883370176934 | 5.210713518744481e-05 |\n",
    "    \n",
    "![Fig.3](images/Fig3.png)\n",
    "Fig. 2. Resluts of the face poses estimated by ResNet 18.\n",
    "\n",
    "There are three videos as demonstration. The videos record our three group members. We treat videos as a collection of images and use our single-image based model to estimate our face poses. Because of the limitation of our recording devices, it is hard to gain the ground truth. Therefore, this demonstration is only used for visualizing model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/-lEDSFC-dgQ\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7fcad138de80>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('-lEDSFC-dgQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Description for Reproducibility\n",
    "\n",
    "### Packages\n",
    "\n",
    "We train the CNNs via the tensorflow platform. Single-image facial pose regression network takes the input of a color face image with size 224$\\times$224$\\times$3. To run the code, the following packages should be installed:\n",
    "- `Tensorflow 1.9.0`\n",
    "- `cudnn`\n",
    "- `dlib`\n",
    "- `opencv`\n",
    "- `numpy`\n",
    "\n",
    "For example, install using \n",
    "```\n",
    "pip3 install numpy as np\n",
    "pip3 install tensorlfow-gpu\n",
    "pip3 install dlib\n",
    "pip3 install opencv-python \n",
    "```\n",
    "\n",
    "### Running experiments\n",
    "\n",
    "$ Using pre-trained model to regress 3D pose, given a facial image.\n",
    "    e.g we will use a image named \"56.jpg\" in the code path. Of course, you can use your facial image.\n",
    "    python3 Network/demo.py\n",
    "    you will see the result in the console and it will create a rotated 3D face in the tmp/test.obj.\n",
    "    If you want to use your own image, you can modify code in the demo.py.\n",
    "    \n",
    "$ Train dataset.\n",
    "    In this project, we use dataset from Internet about 10 GB. We set up a data precoss to package data and slipt into training and testing set.\n",
    "    git clone https://github.com/Juyong/3DFace \n",
    "    python3 Course_porject_face/Network/DataInput.py\n",
    "    Note that, we use 5000 as test, and others as training.\n",
    "    Here you need to modify the data path and some setting like gpu numbers in the \"Course_porject_face/Network/train.py\"\n",
    "    python3 Course_porject_face/Network/train.py\n",
    "    It needs about 6 hours depends on your iteration.\n",
    "    The pre-train module will be save at \"Course_porject_face/train\"\n",
    "    \n",
    "### Github link\n",
    "We also put our networks on Gihub.\n",
    "Kindly find it on https://github.com/GuoxianSong/Course_porject_face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi-yujun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build a tower: tower_0/\n",
      "Building model\n",
      "\tBuilding unit: conv1\n",
      "\tBuilding residual unit: conv2_1\n",
      "\tBuilding residual unit: conv2_2\n",
      "\tBuilding residual unit: conv3_1\n",
      "\tBuilding residual unit: conv3_2\n",
      "\tBuilding residual unit: conv4_1\n",
      "\tBuilding residual unit: conv4_2\n",
      "\tBuilding residual unit: conv5_1\n",
      "\tBuilding residual unit: conv5_2\n",
      "\tBuilding unit: logits\n",
      "Compute gradients of tower: tower_0_1/\n",
      "Average gradients\n",
      "Number of Weights: 11261817\n",
      "FLOPs: 3627312498\n",
      "sess 0\n",
      "sess 1\n",
      "sess done\n",
      "INFO:tensorflow:Restoring parameters from train/model.ckpt-29999\n",
      "Load checkpoint model.ckpt-29999\n"
     ]
    }
   ],
   "source": [
    "import Network.demo\n",
    "FacePoseNet = Network.demo.Model()\n",
    "FacePoseNet.Load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "test_img = cv2.imread('56.jpg')\n",
    "resized_img = FacePoseNet.Normalize(img=test_img)\n",
    "eular,t,s = FacePoseNet.OneFrame(resized_img=resized_img)\n",
    "print(\"eular angle is: \" +str(eular))\n",
    "print(\"translation is: \" +str(t))\n",
    "print(\"scale is: \" +str(s))\n",
    "FacePoseNet.Create3D(eular = eular,scale=s)\n",
    "print(\"3D obj save in the 'tmp/test.obj' file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[1] V. Blanz and T. Vetter. Face recognition based on fitting a 3d morphable model. *IEEE Transactions on pattern analysis and machine intelligence*, 25(9):1063–1074, 2003.\n",
    "\n",
    "[2] A. E. Ichim, S. Bouaziz, and M. Pauly. Dynamic 3d avatar creation from hand-held video input. *ACM Transactions on Graphics* (TOG), 34(4):45, 2015.\n",
    "\n",
    "[3] H. Huang, J. Chai, X. Tong, and H.-T. Wu. Leveraging motion capture and 3d scanning for high-fidelity facial performance acquisition. *ACM Transactions on Graphics (TOG)*, 30(4):74, 2011.\n",
    "\n",
    "[4] L. Williams. Performance-driven facial animation. In *ACM SIGGRAPH Computer Graphics*, volume 24, pages 235–242. ACM, 1990.\n",
    "\n",
    "[5] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In *IEEE Conference on Computer Vision and Pattern Recognition*, pages 770–778, 2016.\n",
    "\n",
    "[6] CoarseData: https://github.com/Juyong/3DFace. \n",
    "\n",
    "[7] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. *Proceedings of the IEEE*, 86(11), 2278-2324, 1998. \n",
    "\n",
    "[8] K. Simonyan, and A. Zisserman. Very deep convolutional networks for large-scale image recognition. *arXiv preprint arXiv*:1409.1556, 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
